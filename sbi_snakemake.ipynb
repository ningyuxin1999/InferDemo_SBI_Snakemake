{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5381166b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Simulation-Based Inference (SBI) in population genetics\n",
    "\n",
    "Welcome to this workshop on applying neural posterior estimation (NPE) in population genetics! In this notebook, we will explore together how to use our Snakemake pipeline for simulation-based inference in population genetics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61f0e33",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "We'll walk you through:\n",
    "1. A brief overview of the SBI toolbox and NPE.\n",
    "2. Setting up the required environments? (probably should do this in advance)\n",
    "3. Reading in and exploring pre-simulated data.\n",
    "4. Training a posterior using the SBI toolbox.\n",
    "5. Evaluation of the posterior distribution (based on insufficient dataset).\n",
    "6. Loading a pre-trained posterior with evaluation.\n",
    "7. Visualisations\n",
    "\n",
    "   ...more ideas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04370717",
   "metadata": {},
   "source": [
    "### 1.1. A Brief Overview\n",
    "\n",
    "Neural posterior estimation (NPE) is provided within [sbi toolbox](https://github.com/sbi-dev/sbi) where we can learn the posterior distribution of parameters given observations using flexible neural networks. \n",
    "- It allows us to infer complex, high-dimensional parameters without relying on approximate likelihoods.\n",
    "- The approach is especially useful for scenarios where the likelihood function is expensive or intractable, but data simulation is feasible.\n",
    "  \n",
    "You can visit [sbi documentation](https://sbi-dev.github.io/sbi/latest/) for more information.\n",
    "\n",
    "Based on sbi, our [Snakemake](https://snakemake.readthedocs.io/en/stable/) pipeline provides a framework for simulation-based inference in population genetics using [msprime](https://tskit.dev/msprime/docs/stable/quickstart.html). It automates data simulation (e.g., tree sequences), training of neural posterior estimators (NPEs), and plotting/visualization of inferred parameters. \n",
    "\n",
    "Three different workflows are provided: an amortized msprime workflow, an amortized dadi workflow, and a sequential msprime workflow. Configuration files control the number of simulations, model details, and training settings, making the workflow flexible for various population genetic scenarios.\n",
    "For more information on this pipeline, please visit our [GitHub repository](https://github.com/your-org/your-sbi-snakemake-pipeline).\n",
    "\n",
    "- [ ] Make a few slides introducing the genral idea of NPE (to biologists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473c2e1",
   "metadata": {},
   "source": [
    "### 1.2. Prerequisites\n",
    "\n",
    "Before we begin, ensure the following:\n",
    "1. **Operating System**: Linux/macOS/Windows (with WSL2 or an equivalent environment).\n",
    "2. **Hardware**:\n",
    "    - Only CPU is needed for this workshop.\n",
    "    - [ ] GPU usage will be provided in another Notebook.\n",
    "3. **Software**:\n",
    "    - Python 3.9+ [sbi0.22.0](https://github.com/sbi-dev/sbi/releases/tag/v0.22.0).\n",
    "    - [conda](https://docs.conda.io/en/latest/) (or `venv`) for environment management.\n",
    "    - Required Python libraries for this tutorial ([requirements](https://github.com/kr-colab/popgensbi_snakemake/blob/main/requirements.yaml)).\n",
    "\n",
    "#### Environment Setup\n",
    "\n",
    "To run this notebook, please follow these steps:\n",
    "1. Install [conda](https://docs.conda.io/en/latest/miniconda.html) if you haven’t already.\n",
    "2. Clone the repository: `git clone https://github.com/kr-colab/popgensbi_snakemake.git`\n",
    "3. Download the .zip folder `folder_name` for testing data and pre-trained neural networks.\n",
    "4. Create the environment: `conda env create -f requirements.yaml`\n",
    "5. Activate the environment: `conda activate popgensbi_env`\n",
    "6. Launch Jupyter notebook: `jupyter notebook`.\n",
    "7. In the Notebook, select the \"popgensbi\" kernel if prompted.\n",
    "\n",
    "### 1.3. Environment Test\n",
    "\n",
    "- [ ] Here should be a short test block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af48ef9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: The following packages are missing: ['dadi']\n",
      "Please install or switch to the conda environment that has them.\n"
     ]
    }
   ],
   "source": [
    "# Are you ready to go?\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# List of critical packages we expect\n",
    "required_packages = [\"snakemake\", \"msprime\", \"dadi\", \"sbi\", \"torch\"]\n",
    "missing_packages = []\n",
    "\n",
    "for pkg in required_packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        missing_packages.append(pkg)\n",
    "\n",
    "if missing_packages:\n",
    "    print(\"WARNING: The following packages are missing:\", missing_packages)\n",
    "    print(\"Please install or switch to the conda environment that has them.\")\n",
    "else:\n",
    "    print(\"All required packages found. Environment looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc44324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if NPE is running without problem.\n",
    "import torch\n",
    "from sbi.inference import NPE\n",
    "\n",
    "# define shifted Gaussian simulator.\n",
    "def simulator(θ): return θ + torch.randn_like(θ)\n",
    "# draw parameters from Gaussian prior.\n",
    "θ = torch.randn(1000, 2)\n",
    "# simulate data\n",
    "x = simulator(θ)\n",
    "\n",
    "# choose sbi method and train\n",
    "inference = NPE()\n",
    "inference.append_simulations(θ, x).train()\n",
    "\n",
    "# do inference given observed data\n",
    "x_o = torch.ones(2)\n",
    "posterior = inference.build_posterior()\n",
    "samples = posterior.sample((1000,), x=x_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d46c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Explore demographic inference\n",
    "\n",
    "In this section, we first have a look at the data set. \n",
    "All data provided here are from testing sets so that you can test them on a pre-trained neural network to make your own posterior samples.\n",
    "\n",
    "### 2.1 Simulated data\n",
    "\n",
    "Through a prior, we randomly generated some demographic scenarios. Each scenario has 21 effective population sizes from presence to the past along exponentially growing gaps, and a recombination rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71f068ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804a005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_population_sizes = '... .tsv'\n",
    "# df_pop_size = pd.read_csv(path_population_sizes, sep='\\t')\n",
    "# df_pop_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d28fca1",
   "metadata": {},
   "source": [
    "### 2.2 SNP matrix\n",
    "\n",
    "[Msprime](https://tskit.dev/msprime/docs/stable/quickstart.html) is a powerful coalescent simulator that models the ancestry of a sample of genomes under specified demographic parameters (e.g., population sizes, mutation rates, recombination rates, and population splits). It generates a [tree sequence](https://tskit.dev/tutorials/intro.html), which is essentially a record of how all sampled individuals coalesce back to their common ancestors. By placing mutations along the branches of these ancestral trees (according to the specified mutation rate), msprime outputs simulated genetic variation—ultimately yielding SNP data or variant matrices that can be used for downstream analyses, such as training neural posterior estimators in our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ada2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNP matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927dc707",
   "metadata": {},
   "source": [
    "### 2.3 Summary statistics\n",
    "\n",
    "We computed site frequency spectrum and linkage disequilibrium from the SNP matrices, which ends up into 68 summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe58d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_summary_statistics = '.tsv'\n",
    "# df_sum = pd.read_csv(path_summary_statistics, sep='\\t')\n",
    "# df_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355500ff",
   "metadata": {},
   "source": [
    "### 2.4 Pre-trained network\n",
    "\n",
    "Give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "## This network is trained on population sizes and summary statistics. \n",
    "# posterior_path = 'pretrained_posterior.pkl'\n",
    "# with open(posterior_path, 'rb') as f:\n",
    "#     pretrained_posterior = pickle.load(f)\n",
    "\n",
    "# # Now we can do inference with the loaded posterior\n",
    "# test_index = 1\n",
    "# observed_x = x[test_index].unsqueeze(0)\n",
    "# true_params = theta[test_index]\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     inferred_samples = pretrained_posterior.sample((1000,), x=observed_x)\n",
    "# inferred_mean = inferred_samples.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5523fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aebe6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is how the network is trained. Uncomment to try it out. This could take some time.\n",
    "\n",
    "# from sbi.inference import SNPE, prepare_for_sbi\n",
    "# import torch\n",
    "\n",
    "# # Convert to torch tensors\n",
    "# theta = torch.tensor(df[...].values, dtype=torch.float32)\n",
    "# x = torch.tensor(df[...].values, dtype=torch.float32)\n",
    "\n",
    "# inference = SNPE(prior=None)  # Usually, you'd define a prior or pass a prior object.\n",
    "\n",
    "# # Train the posterior (this can take a while, especially on CPU)\n",
    "# density_estimator = inference.append_simulations(theta, x).train()\n",
    "# posterior = inference.build_posterior(density_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604bba44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449ac06d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Various scenarios\n",
    "\n",
    "A population could go through different types of history. We can specifically simulate them here, and see how the neural network performs on each of them. In this section, you will be simulating the testing data and computing the summary statistics on your own, instead of loading them directly! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788a9943",
   "metadata": {},
   "source": [
    "### 3.1 Simulate 6 representative scenarios\n",
    "\n",
    "- **Medium**: constant population sizes of 5,000.\n",
    "- **Large**: constant population sizes of 50,000. \n",
    "- **Decline**: decreasing population sizes. \n",
    "- **Expansion**: increasing population sizes.\n",
    "- **Bottleneck**: enlarging population sizes followed by a \n",
    "- **Zigzag**: two bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0378a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_scenario(population_size, population_time, seed, num_replicates, mutation_rate, recombination_rate, \n",
    "                      segment_length, num_sample):\n",
    "    \n",
    "    demography = msprime.Demography()\n",
    "    demography.add_population(initial_size=population_size[0])\n",
    "\n",
    "    for i in range(1, len(population_size)):\n",
    "        demography.add_population_parameters_change(time=population_time[i], initial_size=population_size[i], growth_rate=0)\n",
    "\n",
    "    ts = msprime.sim_ancestry(\n",
    "        num_sample,\n",
    "        random_seed=seed,\n",
    "        sequence_length=segment_length,\n",
    "        ploidy=1,\n",
    "        num_replicates=num_replicates,\n",
    "        demography=demography,\n",
    "        recombination_rate=recombination_rate)\n",
    "    pos = []\n",
    "    snp = []\n",
    "    \n",
    "    for rep, tree in enumerate(ts):\n",
    "        mts = msprime.sim_mutations(tree, rate=mutation_rate, random_seed=seed)\n",
    "        positions = [variant.site.position for variant in mts.variants()]\n",
    "        positions = np.array(positions) - np.array([0] + positions[:-1])\n",
    "        positions = positions.astype(int)\n",
    "        pos.append(positions)\n",
    "        SNPs = mts.genotype_matrix().T.astype(np.uint8)\n",
    "        snp.append(SNPs)\n",
    "    \n",
    "    data = [[snp[i], pos[i]] for i in range(len(snp))]\n",
    "    data = [np.vstack([d[1], d[0]]) for d in data]\n",
    "    return data\n",
    "\n",
    "# Population sizes are defined on a log10 scale\n",
    "scenarios = {'Medium': [3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7, 3.7],   \n",
    "             'Large': 4.7 * np.ones(shape=21, dtype='float'), \n",
    "             'Decline': [2.5, 2.5, 3, 3, 3, 3, 3.2, 3.4, 3.6, 3.8, 4, 4.2, 4.6, 4.6, 4.6, 4.6, 4.6, 4.6, 4.6, 4.6, 4.6], \n",
    "             'Expansion': [4.7, 4.7, 4.7, 4.6, 4.6, 4.5, 4.4, 4.3, 4, 3.7, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4, 3.4], \n",
    "             'Bottleneck': [4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.5, 4.15, 3.8, 4.3, 4.8, 4.55, 4.3, 4.05, 3.8, 3.8, 3.8, 3.8], \n",
    "             'Zigzag': [4.8, 4.8, 4.8, 4.5, 4.15, 3.8, 4.15, 4.5, 4.8, 4.5, 4.15, 3.8, 4.3, 4.8, 4.55, 4.3, 4.05, 3.8, 3.8, 3.8, 3.8]}\n",
    "scenarios = {k:10**np.array(scenarios[k]) for k in scenarios.keys()}\n",
    "\n",
    "seed = 2\n",
    "num_replicates = 100\n",
    "mutation_rate = 1e-8\n",
    "segment_length = 2e6\n",
    "time_rate = 0.06\n",
    "tmax = 130000\n",
    "num_time_windows = 21\n",
    "num_sample = 50\n",
    "population_time = [(np.exp(np.log(1 + time_rate * tmax) * i /\n",
    "                  (num_time_windows - 1)) - 1) / time_rate for i in\n",
    "                  range(num_time_windows)]\n",
    "\n",
    "snp_data = {}\n",
    "for k in scenarios.keys():\n",
    "    print(f'Simulating scenario \\\"{k}\\\"')\n",
    "    population_size = scenarios[k]\n",
    "    recombination_rate = np.random.uniform(low=1e-9, high=1e-8)\n",
    "    snp_data[k] = simulate_scenario(population_size, population_time, seed, num_replicates, mutation_rate, recombination_rate, segment_length, num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65248329",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94348cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b338af13",
   "metadata": {},
   "source": [
    "### 3.2 Compute summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LD(haplotype, pos_vec, size_chr, circular=True, distance_bins=None):\n",
    "    if distance_bins is None or isinstance(distance_bins, int):\n",
    "        if isinstance(distance_bins, int):\n",
    "            n_bins = distance_bins - 1\n",
    "        else:\n",
    "            n_bins = 19\n",
    "        if circular:\n",
    "            distance_bins = np.logspace(2, np.log10(size_chr // 2), n_bins)\n",
    "            distance_bins = np.insert(distance_bins, 0, [0])\n",
    "        else:\n",
    "            distance_bins = np.logspace(2, np.log10(size_chr), n_bins)\n",
    "            distance_bins = np.insert(distance_bins, 0, [0])\n",
    "\n",
    "    # Iterate through gap sizes\n",
    "    n_SNP, n_samples = haplotype.shape\n",
    "    gaps = (2 ** np.arange(0, np.log2(n_SNP), 1)).astype(int)\n",
    "\n",
    "    # Initialize lists to store selected SNP pairs and LD values\n",
    "    selected_snps = []\n",
    "    for gap in gaps:\n",
    "        snps = np.arange(0, n_SNP, gap) + np.random.randint(0, (n_SNP - 1) % gap + 1)\n",
    "        # adding a random start (+1, bc 2nd bound in randint is exlusive)\n",
    "\n",
    "        # non overlapping contiguous pairs\n",
    "        # snps=[ 196, 1220, 2244] becomes\n",
    "        # snp_pairs=[(196, 1220), (1221, 2245)]\n",
    "        snp_pairs = np.unique([((snps[i] + i) % n_SNP, (snps[i + 1] + i) % n_SNP) for i in range(len(snps) - 1)],\n",
    "                              axis=0)\n",
    "\n",
    "        # If we don't have enough pairs (typically when gap is large), we add a random rotation until we have at\n",
    "        # least 300) count = 0\n",
    "\n",
    "        if not circular:\n",
    "            snp_pairs = snp_pairs[snp_pairs[:, 0] < snp_pairs[:, 1]]\n",
    "        last_pair = snp_pairs[-1]\n",
    "\n",
    "        if circular:\n",
    "            max_value = n_SNP - 1\n",
    "        else:\n",
    "            max_value = n_SNP - gap - 1\n",
    "\n",
    "        while len(snp_pairs) <= min(300, max_value):\n",
    "            # count += 1 if count % 10 == 0: print(\">>  \" + str(gap) + \" - \" + str(len(np.unique(snp_pairs,\n",
    "            # axis=0))) + \" -- \"+ str(len(snps) - 1) + \"#\" + str(count)) remainder = (n_SNP - 1) % gap if (n_SNP - 1)\n",
    "            # % gap != 0 else (n_SNP - 1) // gap\n",
    "            random_shift = np.random.randint(1, n_SNP) % n_SNP\n",
    "            new_pair = (last_pair + random_shift) % n_SNP\n",
    "            snp_pairs = np.unique(np.concatenate([snp_pairs, new_pair.reshape(1, 2)]), axis=0)\n",
    "            last_pair = new_pair\n",
    "\n",
    "            if not circular:\n",
    "                snp_pairs = snp_pairs[snp_pairs[:, 0] < snp_pairs[:, 1]]\n",
    "\n",
    "        selected_snps.append(snp_pairs)\n",
    "\n",
    "    # Functions to aggregate the values within each distance bin\n",
    "    agg_bins = {\"snp_dist\": [\"mean\"], \"r2\": [\"mean\", \"count\", \"sem\"]}\n",
    "\n",
    "    ld = pd.DataFrame()\n",
    "    for i, snps_pos in enumerate(selected_snps):\n",
    "\n",
    "        if circular:\n",
    "            sd = pd.DataFrame((np.diff(pos_vec[snps_pos]) % size_chr) % (size_chr // 2),\n",
    "                              columns=[\"snp_dist\"])  # %size_chr/2 because max distance btw 2 SNP is size_chr/2\n",
    "        else:\n",
    "            sd = pd.DataFrame((np.diff(pos_vec[snps_pos])), columns=[\"snp_dist\"])\n",
    "\n",
    "        sd[\"dist_group\"] = pd.cut(sd.snp_dist, bins=distance_bins)\n",
    "        sr = [allel.rogers_huff_r(snps) ** 2 for snps in haplotype[snps_pos]]\n",
    "        sd[\"r2\"] = sr\n",
    "        sd[\"gap_id\"] = i\n",
    "        ld = pd.concat([ld, sd])\n",
    "\n",
    "    ld2 = ld.dropna().groupby(\"dist_group\").agg(agg_bins)\n",
    "\n",
    "    # Flatten the MultiIndex columns and rename explicitly\n",
    "    ld2.columns = ['_'.join(col).strip() for col in ld2.columns.values]\n",
    "    ld2 = ld2.rename(columns={\n",
    "        'snp_dist_mean': 'mean_dist',\n",
    "        'r2_mean': 'mean_r2',\n",
    "        'r2_count': 'Count',\n",
    "        'r2_sem': 'sem_r2'\n",
    "    })\n",
    "    # ld2 = ld2.fillna(-1)\n",
    "    return ld2[['mean_dist', 'mean_r2', 'Count', 'sem_r2']]\n",
    "\n",
    "\n",
    "def sfs(haplotype, ac):\n",
    "    \"\"\"\n",
    "    Calculate the site frequency spectrum (SFS) from haplotype data and allele counts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    haplotype (numpy.ndarray): The haplotype matrix where rows represent variants and columns represent individuals.\n",
    "    ac (numpy.ndarray): Allele count array where each entry represents the count of the derived allele at a site.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame: DataFrame containing the SFS. Each row corresponds to a frequency (number of individuals),\n",
    "    with the corresponding count of SNPs that have that frequency.\n",
    "\n",
    "    \"\"\"\n",
    "    nindiv = haplotype.shape[1]\n",
    "    tmp_df = pd.DataFrame({\"N_indiv\": range(1, nindiv)})\n",
    "\n",
    "    # getting unfolded sfs\n",
    "    df_sfs = pd.DataFrame(allel.sfs(ac.T[1]), columns=[\"count_SNP\"])\n",
    "    df_sfs.index.name = \"N_indiv\"\n",
    "    df_sfs.reset_index(inplace=True)\n",
    "    df_sfs = df_sfs.merge(tmp_df, on=\"N_indiv\", how=\"right\").fillna(0).astype(int)\n",
    "\n",
    "    return df_sfs\n",
    "\n",
    "def compute_sumstat(snp_data):\n",
    "    output=[]\n",
    "    scenario_data = {}\n",
    "    for scenario in snp_data:\n",
    "        for rep in snp_data[scenario]:\n",
    "            snp = rep[1:][:50,:400]\n",
    "            pos = rep[0][:400]\n",
    "\n",
    "            if any(np.diff(pos) < 0):\n",
    "                pos = np.cumsum(pos)\n",
    "            if pos.max() <= 1:\n",
    "                pos = (pos * 2e6).round().astype(int)\n",
    "\n",
    "            haplotype = allel.HaplotypeArray(snp.T)\n",
    "            allel_count = haplotype.count_alleles()\n",
    "        \n",
    "            afs = sfs(haplotype, allel_count)\n",
    "            afs = afs.set_index('N_indiv')\n",
    "            afs['scenario'] = scenario\n",
    "\n",
    "            ld = LD(haplotype, pos, circular=False, size_chr=2e6)\n",
    "            ld[\"scenario\"] = scenario\n",
    "            ld = ld.drop(columns=['sem_r2'])\n",
    "\n",
    "            if scenario not in scenario_data:\n",
    "                scenario_data[scenario] = {\"afs\": [], \"ld\": []}\n",
    "            scenario_data[scenario][\"afs\"].append(afs)\n",
    "            scenario_data[scenario][\"ld\"].append(ld)\n",
    "\n",
    "    for scenario in snp_data:\n",
    "        mean_afs = pd.concat(scenario_data[scenario][\"afs\"]).groupby(\"N_indiv\").mean()\n",
    "        mean_afs['scenario'] = scenario\n",
    "        mean_afs.reset_index(inplace=True)\n",
    "        mean_ld = pd.concat(scenario_data[scenario][\"ld\"]).groupby(\"dist_group\").mean()\n",
    "        mean_ld['scenario']=scenario\n",
    "        mean_ld.reset_index(inplace=True)\n",
    "        \n",
    "        df_sfs = mean_afs.set_index('N_indiv')\n",
    "        df_sfs_out = df_sfs.loc[df_sfs['scenario'] == scenario]\n",
    "        df_sfs_out = df_sfs_out.drop(columns=['scenario'])\n",
    "        df_sfs_out = df_sfs_out.stack(dropna=False)\n",
    "        df_sfs_out.index = df_sfs_out.index.map('{0[1]}_{0[0]}'.format)\n",
    "        df_sfs_out = df_sfs_out.to_frame().T\n",
    "        df_sfs_out = df_sfs_out.set_index([[scenario]])\n",
    "\n",
    "        df_ld_out = mean_ld.loc[np.array(mean_ld['scenario'] == scenario)]\n",
    "        df_ld_out = df_ld_out.drop(columns=['scenario'])\n",
    "        df_ld_out = df_ld_out.stack(dropna=False)\n",
    "        df_ld_out.index = df_ld_out.index.map('{0[1]}_{0[0]}'.format)\n",
    "        df_ld_out = df_ld_out.to_frame().T\n",
    "        df_ld_out = df_ld_out.set_index([[scenario]])\n",
    "        df = pd.merge(df_sfs_out, df_ld_out, left_index=True, right_index=True)\n",
    "        output.append(df)\n",
    "    return output\n",
    "\n",
    "sumstats=compute_sumstat(snp_data)\n",
    "tt = pd.DataFrame()\n",
    "for v in sumstats:\n",
    "    tt = pd.concat([tt,v])\n",
    "sumstats = tt.drop(columns=[c for c in tt.columns if c.startswith('mean_dist') or c.startswith('dist') or c.startswith('Count_')])\n",
    "sumstats.index.name='scenario'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b8a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumstats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e988f87",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "So far we have walked through the complete workflow including data simulation and NPE training. \n",
    "There are different ways to visualize the posterior distribution using sbi integrated, or self-defined functions.\n",
    "...\n",
    "\n",
    "## 4. Evaluation and visualisation\n",
    "\n",
    "This is a free-styling section! Apart from some functions provided here, please try visualize the results by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93628efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the summary stats for testing 6 scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4366c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5817aad7",
   "metadata": {},
   "source": [
    "Thank you for following along! We hope this tutorial helps you get started with the SBI Snakemake pipeline for population genetics.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. More fancy applications ...\n",
    "\n",
    "- [ ] Andy's pipeline, a brief introduction of what can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb83abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi_env",
   "language": "python",
   "name": "sbi_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
